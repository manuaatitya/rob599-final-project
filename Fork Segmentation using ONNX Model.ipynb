{"cells":[{"cell_type":"code","execution_count":null,"id":"901c8ef3","metadata":{"id":"901c8ef3"},"outputs":[],"source":["# Copyright (c) Meta Platforms, Inc. and affiliates."]},{"cell_type":"markdown","id":"1662bb7c","metadata":{"id":"1662bb7c"},"source":["# Produces masks from prompts using an ONNX model"]},{"cell_type":"markdown","id":"7fcc21a0","metadata":{"id":"7fcc21a0"},"source":["SAM's prompt encoder and mask decoder are very lightweight, which allows for efficient computation of a mask given user input. This notebook shows an example of how to export and use this lightweight component of the model in ONNX format, allowing it to run on a variety of platforms that support an ONNX runtime."]},{"cell_type":"markdown","id":"55ae4e00","metadata":{"id":"55ae4e00"},"source":["## Environment Set-up"]},{"cell_type":"markdown","id":"109a5cc2","metadata":{"id":"109a5cc2"},"source":["If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. The latest stable versions of PyTorch and ONNX are recommended for this notebook. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."]},{"cell_type":"code","execution_count":null,"id":"39b99fc4","metadata":{"id":"39b99fc4"},"outputs":[],"source":["using_colab = True"]},{"cell_type":"code","execution_count":null,"id":"296a69be","metadata":{"id":"296a69be","executionInfo":{"status":"ok","timestamp":1682372837537,"user_tz":240,"elapsed":47995,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"e16bc91b-0b08-4086-d687-07984c8e6c8f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.0.0+cu118\n","Torchvision version: 0.15.1+cu118\n","CUDA is available: True\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n","Collecting onnx\n","  Downloading onnx-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime\n","  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx) (4.5.0)\n","Collecting coloredlogs\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (1.11.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (23.3.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting humanfriendly>=9.1\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.13.1 onnxruntime-1.14.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-cx2rasgy\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-cx2rasgy\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 567662b0fd33ca4b022d94d3b8de896628cd32dd\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: segment-anything\n","  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36610 sha256=1ea577af0fe2e39b65cbf4cad888f5540b0a1459deb2bfba7ba18e94cf115482\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0sk21ar8/wheels/d5/11/03/7aca746a2c0e09f279b10436ced7175926bc38f650b736a648\n","Successfully built segment-anything\n","Installing collected packages: segment-anything\n","Successfully installed segment-anything-1.0\n","--2023-04-24 21:47:01--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.22.89, 13.226.22.91, 13.226.22.68, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.22.89|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   235MB/s    in 11s     \n","\n","2023-04-24 21:47:13 (218 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting piexif\n","  Downloading piexif-1.1.3-py2.py3-none-any.whl (20 kB)\n","Installing collected packages: piexif\n","Successfully installed piexif-1.1.3\n"]}],"source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n","    \n","    !mkdir images\n","    # !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n","        \n","    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","    !pip install piexif"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"2UEdi1ZwmEy7"},"id":"2UEdi1ZwmEy7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"3ZRELlSzox3I","executionInfo":{"status":"ok","timestamp":1682372909639,"user_tz":240,"elapsed":72111,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"e802541b-40c5-47f9-cb50-7d00744e1ab1","colab":{"base_uri":"https://localhost:8080/"}},"id":"3ZRELlSzox3I","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"dc4a58be","metadata":{"id":"dc4a58be"},"source":["## Set-up"]},{"cell_type":"markdown","id":"42396e8d","metadata":{"id":"42396e8d"},"source":["Note that this notebook requires both the `onnx` and `onnxruntime` optional dependencies, in addition to `opencv-python` and `matplotlib` for visualization."]},{"cell_type":"code","execution_count":null,"id":"2c712610","metadata":{"id":"2c712610"},"outputs":[],"source":["import torch\n","import numpy as np\n","import piexif\n","import cv2\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from segment_anything import sam_model_registry, SamPredictor\n","from segment_anything.utils.onnx import SamOnnxModel\n","\n","import onnxruntime\n","from onnxruntime.quantization import QuantType\n","from onnxruntime.quantization.quantize import quantize_dynamic"]},{"cell_type":"code","execution_count":null,"id":"f29441b9","metadata":{"id":"f29441b9"},"outputs":[],"source":["def show_mask(mask, ax):\n","    color = np.array([255/255, 255/255, 255/255, 0.7])\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","    \n","def show_points(coords, labels, ax, marker_size=375):\n","    pos_points = coords[labels==1]\n","    neg_points = coords[labels==0]\n","    # ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","    # ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n","    \n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))   "]},{"cell_type":"markdown","id":"bd0f6b2b","metadata":{"id":"bd0f6b2b"},"source":["## Export an ONNX model"]},{"cell_type":"markdown","id":"1540f719","metadata":{"id":"1540f719"},"source":["Set the path below to a SAM model checkpoint, then load the model. This will be needed to both export the model and to calculate embeddings for the model."]},{"cell_type":"code","execution_count":null,"id":"76fc53f4","metadata":{"id":"76fc53f4"},"outputs":[],"source":["checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\""]},{"cell_type":"code","execution_count":null,"id":"11bfc8aa","metadata":{"id":"11bfc8aa"},"outputs":[],"source":["sam = sam_model_registry[model_type](checkpoint=checkpoint)"]},{"cell_type":"markdown","id":"450c089c","metadata":{"id":"450c089c"},"source":["The script `segment-anything/scripts/export_onnx_model.py` can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter `return_single_mask=True`."]},{"cell_type":"code","execution_count":null,"id":"38a8add8","metadata":{"id":"38a8add8"},"outputs":[],"source":["onnx_model_path = None  # Set to use an already exported model, then skip to the next section."]},{"cell_type":"code","execution_count":null,"id":"7da638ba","metadata":{"scrolled":false,"id":"7da638ba","executionInfo":{"status":"ok","timestamp":1682372931932,"user_tz":240,"elapsed":5916,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"413ab92a-40e6-413f-e606-2b6a512aa44b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n"]}],"source":["import warnings\n","\n","onnx_model_path = \"sam_onnx_example.onnx\"\n","\n","onnx_model = SamOnnxModel(sam, return_single_mask=True)\n","\n","dynamic_axes = {\n","    \"point_coords\": {1: \"num_points\"},\n","    \"point_labels\": {1: \"num_points\"},\n","}\n","\n","embed_dim = sam.prompt_encoder.embed_dim\n","embed_size = sam.prompt_encoder.image_embedding_size\n","mask_input_size = [4 * x for x in embed_size]\n","dummy_inputs = {\n","    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n","    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n","    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n","    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n","    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n","    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n","}\n","output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n","\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    with open(onnx_model_path, \"wb\") as f:\n","        torch.onnx.export(\n","            onnx_model,\n","            tuple(dummy_inputs.values()),\n","            f,\n","            export_params=True,\n","            verbose=False,\n","            opset_version=17,\n","            do_constant_folding=True,\n","            input_names=list(dummy_inputs.keys()),\n","            output_names=output_names,\n","            dynamic_axes=dynamic_axes,\n","        )    "]},{"cell_type":"markdown","id":"c450cf1a","metadata":{"id":"c450cf1a"},"source":["If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise."]},{"cell_type":"code","execution_count":null,"id":"235d39fe","metadata":{"id":"235d39fe","executionInfo":{"status":"ok","timestamp":1682372932817,"user_tz":240,"elapsed":887,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"039201e6-8aef-4b54-a1a7-2dbfd06d7fb3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Ignore MatMul due to non constant B: /[/transformer/layers.0/self_attn/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.0/self_attn/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_token_to_image/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_token_to_image/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_image_to_token/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_image_to_token/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/self_attn/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/self_attn/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_token_to_image/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_token_to_image/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_image_to_token/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_image_to_token/MatMul_1]\n","Ignore MatMul due to non constant B: /[/transformer/final_attn_token_to_image/MatMul]\n","Ignore MatMul due to non constant B: /[/transformer/final_attn_token_to_image/MatMul_1]\n","Ignore MatMul due to non constant B: /[/MatMul_1]\n"]}],"source":["onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\"\n","quantize_dynamic(\n","    model_input=onnx_model_path,\n","    model_output=onnx_model_quantized_path,\n","    optimize_model=True,\n","    per_channel=False,\n","    reduce_range=False,\n","    weight_type=QuantType.QUInt8,\n",")\n","onnx_model_path = onnx_model_quantized_path"]},{"cell_type":"markdown","id":"927a928b","metadata":{"id":"927a928b"},"source":["## Example Image"]},{"cell_type":"code","source":["import glob as gl\n","files = gl.glob('/content/drive/MyDrive/NeRF Supervision/nerf-supervision-public/data/fork/images/*.png')"],"metadata":{"id":"QZzUKH8rz9si"},"id":"QZzUKH8rz9si","execution_count":null,"outputs":[]},{"cell_type":"code","source":["counter = 0\n","\n","for item in sorted(files):\n","  counter += 1\n","  print(item)\n","  image = cv2.imread(item)\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","  im = Image.open(item)\n","  metadata = im.info\n","\n","  ort_session = onnxruntime.InferenceSession(onnx_model_path)\n","  sam.to(device='cuda')\n","  predictor = SamPredictor(sam)\n","  predictor.set_image(image)\n","  image_embedding = predictor.get_image_embedding().cpu().numpy()\n","\n","  input_box = np.array([0, 0, 4032, 3024])\n","  input_point = np.array([[575, 750]])\n","  input_label = np.array([0])\n","\n","  onnx_box_coords = input_box.reshape(2, 2)\n","  onnx_box_labels = np.array([2,3])\n","\n","  onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\n","  onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n","\n","  onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n","\n","  onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n","  onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n","\n","  ort_inputs = {\n","      \"image_embeddings\": image_embedding,\n","      \"point_coords\": onnx_coord,\n","      \"point_labels\": onnx_label,\n","      \"mask_input\": onnx_mask_input,\n","      \"has_mask_input\": onnx_has_mask_input,\n","      \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n","  }\n","\n","  masks, _, _ = ort_session.run(None, ort_inputs)\n","  masks = masks > predictor.model.mask_threshold\n","\n","  repeat_mask = masks[0,0,:,:,np.newaxis].repeat(3,axis=2)\n","\n","  masked_img = np.where(repeat_mask,np.zeros_like(repeat_mask),image)\n","  # print(im)\n","  # masked_img = image\n","  plt.imshow(masked_img)\n","  plt.show()\n","\n","  modified_img = Image.fromarray(np.uint8(masked_img))\n","  modified_img.info.update(metadata)\n","  modified_img.save('/content/drive/MyDrive/NeRF Supervision/Segmented_fork/seg_fork/{:4d}.png'.format(2000+counter))\n","\n","  # plt.figure(figsize=(40, 30))\n","  # plt.imshow(image)\n","  # show_mask(masks[0], plt.gca())\n","  # plt.axis('off')\n","  # plt.savefig('/content/drive/MyDrive/NeRF Supervision/Segmented_fork/seg_one/{:4d}.png'.format(counter), pil_kwargs={'icc_profile': im.info.get('icc_profile')})\n","  # plt.show()\n","  # break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pCS6Sgi9f65ql4CoQ_wnvjQxdjAbAvOf"},"id":"nS_qdW0e6D08","executionInfo":{"status":"ok","timestamp":1682377734743,"user_tz":240,"elapsed":43241,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"9e11607d-09fe-409c-90ec-e633f47ba67b"},"id":"nS_qdW0e6D08","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["counter = 0\n","\n","for item in files:\n","  counter += 1\n","  image = cv2.imread(item)\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","  im = Image.open(item)\n","  metadata = im.info\n","\n","  ort_session = onnxruntime.InferenceSession(onnx_model_path)\n","  sam.to(device='cuda')\n","  predictor = SamPredictor(sam)\n","  predictor.set_image(image)\n","  image_embedding = predictor.get_image_embedding().cpu().numpy()\n","\n","  input_point = np.array([[500, 375]])\n","  input_label = np.array([1])\n","\n","  onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n","  onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n","\n","  onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n","  onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n","  onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n","  ort_inputs = {\n","    \"image_embeddings\": image_embedding,\n","    \"point_coords\": onnx_coord,\n","    \"point_labels\": onnx_label,\n","    \"mask_input\": onnx_mask_input,\n","    \"has_mask_input\": onnx_has_mask_input,\n","    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n","  }\n","\n","  masks, _, low_res_logits = ort_session.run(None, ort_inputs)\n","  masks = masks > predictor.model.mask_threshold\n","  plt.figure(figsize=(40,30))\n","  plt.imshow(image)\n","  show_mask(masks, plt.gca())\n","  # show_points(input_point, input_label, plt.gca())\n","  plt.axis('off')\n","  plt.savefig('/content/drive/MyDrive/NeRF Supervision/Segmented_fork/realone/{:4d}.png'.format(counter), pil_kwargs={'icc_profile': im.info.get('icc_profile')})\n","  plt.show() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"101nYKB2qKye9IOG50N12zLDEX_hkR39M"},"id":"B5_M9yyf2QMk","executionInfo":{"status":"error","timestamp":1682316751064,"user_tz":240,"elapsed":234935,"user":{"displayName":"Aravind Krishnakumar","userId":"00099289265596889459"}},"outputId":"475b213d-25a1-4323-b4ab-1e8155f55cfe"},"id":"B5_M9yyf2QMk","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb","timestamp":1682315922624}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}